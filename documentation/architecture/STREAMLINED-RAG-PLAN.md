# STREAMLINED RAG PLAN - Insurance Dudes Content System

**Created:** October 7, 2025  
**Goal:** Lean, fast RAG system for viral social media content creation  
**Target Users:** Pakistan-based content team (Urdu input ‚Üí English output)  
**Timeline:** Non-breaking implementation, no disruption to existing workflow

---

## üéØ KEEP vs CUT Analysis

### ‚úÖ KEEP (Essential for Content Team)

1. **Basic Metadata Enhancement** (Phase 1 - Partial)
   - `document_type` (transcript vs document) - ESSENTIAL for filtering
   - `episode_id` (auto-extracted from filename) - ESSENTIAL for episode-specific searches
   - `language` field - ESSENTIAL for Urdu/English handling
   - **WHY:** Content team needs to quickly find specific episodes and filter transcripts vs docs

2. **Simple Hybrid Search** (Phase 2 - Simplified)
   - Vector search (current) + BM25 keyword search
   - **WHY:** "Episode 42" queries need exact keyword matching, not just semantic
   - **SIMPLIFIED:** Remove complex RRF scoring, just combine top results from both

3. **Increased Context Window** (Phase 5 - Partial)
   - Change `topK` from 4 ‚Üí 8 (not 12)
   - **WHY:** More context = better content ideas, but 12 is overkill for social posts

### ‚ùå CUT (Over-Engineered for Viral Content)

1. **Multi-Query Expansion** (Phase 3)
   - Generates 2-3 query variants
   - **WHY CUT:** Adds 100-200ms latency, costs 3x more, content team knows what they're searching for
   - **ALTERNATIVE:** If needed, train team to use better search queries

2. **LLM Re-Ranking** (Phase 4)
   - Uses GPT-5-nano to score and re-rank results
   - **WHY CUT:** Adds 200-300ms latency, increases cost 3.6x, diminishing returns for content extraction
   - **ALTERNATIVE:** Trust hybrid search ranking, let AI agent filter in chat

3. **Complex RRF Scoring** (Phase 2)
   - Reciprocal Rank Fusion with k=60 constant
   - **WHY CUT:** Premature optimization, simpler scoring works fine for <10k documents
   - **ALTERNATIVE:** Simple weighted average: `(vector_score * 0.6) + (bm25_score * 0.4)`

4. **Advanced Metadata Fields** (Phase 1 - Partial)
   - `speaker` field - unused, no speaker diarization in current transcripts
   - `episode_title` auto-extraction - nice-to-have but brittle
   - **WHY CUT:** Not needed now, can add later if transcripts improve

---

## üîß Universal Document Processing

### Adaptive Cleaning Engine

The [`n8n-universal-document-cleaner.js`](n8n-universal-document-cleaner.js) provides **intelligent, format-agnostic** document processing that automatically adapts to ANY input type.

#### Supported Document Types

‚úÖ **Podcast Transcripts**
- Episode delimiters (`-----BEGIN`/`-----END`)
- Speaker labels (`<cite>` tags or `Speaker 1:`)
- Timestamps (`<time>` tags or `00:12:34`)
- Multi-episode files

‚úÖ **PDF Extracts**
- Complex formatting artifacts
- Special characters and ligatures
- Inconsistent whitespace
- Page breaks and headers

‚úÖ **Plain Text Articles**
- Natural paragraph structure
- Minimal formatting
- Standard punctuation

‚úÖ **Email Messages**
- Headers (From, To, Subject)
- Quoted reply chains
- Signatures and disclaimers
- HTML email content

‚úÖ **HTML Documents**
- Tag removal and normalization
- Entity decoding
- Semantic structure preservation

‚úÖ **Markdown Files**
- Header conversion
- Link text extraction
- Code block handling
- Formatting normalization

‚úÖ **Mixed/Unknown Formats**
- Graceful degradation
- Conservative cleaning
- Always produces usable output

#### How It Works: 3-Phase Processing

**Phase 1: Auto-Detection**
```javascript
// Analyzes document structure
{
  hasEpisodeDelimiters: true,
  hasHTMLTags: true,
  hasSpeakerLabels: true,
  hasTimestamps: false,
  hasEmailHeaders: false,
  likelyFormat: 'podcast_transcript',
  confidence: 0.92,
  indicators: ['episode_delimiters', 'html_tags', 'speaker_labels']
}
```

**Phase 2: Adaptive Cleaning**
- Applies cleaning steps **conditionally** based on detection
- Only removes what's detected (no wasted processing)
- Preserves semantic meaning
- Configurable via options

**Phase 3: Intelligent Chunking**
- Respects natural boundaries (episodes, paragraphs, sentences)
- Sentence-aware splitting when possible
- Context preservation with configurable overlap
- Metadata enrichment for each chunk

#### Configuration Options

```javascript
// n8n Code Node Input
{
  text_content: "...",           // Required: Raw document text
  chunk_size: 1000,              // Optional: Target chunk size
  chunk_overlap: 200,            // Optional: Overlap between chunks
  preserve_speakers: false,      // Optional: Keep speaker labels
  preserve_timestamps: false,    // Optional: Keep timestamps
  preserve_episodes: true,       // Optional: Keep episode structure
  aggressive_whitespace: true,   // Optional: Aggressive cleanup
  remove_email_headers: true,    // Optional: Strip email headers
  remove_signatures: true,       // Optional: Remove signatures
  min_confidence: 0.7           // Optional: Minimum detection confidence
}
```

#### Output Format

Each chunk includes rich metadata:

```javascript
{
  json: {
    text_chunk: "Clean, embeddable text content...",
    chunk_index: 0,
    total_chunks: 163,
    detected_format: "podcast_transcript",
    processing_applied: [
      "episode_normalization",
      "html_tags_removed",
      "html_entities_normalized",
      "speaker_labels_removed",
      "aggressive_whitespace",
      "special_chars_normalized"
    ],
    confidence_score: 0.92,
    metadata: {
      original_length: 450123,
      cleaned_length: 387654,
      reduction_percentage: 14,
      episode_id: "episode_0",
      speaker_count: 0,
      timestamp_count: 0
    }
  }
}
```

#### Integration with n8n Workflows

**Workflow: 02 - Document Upload & Vectorization**

1. **Upload Document** ‚Üí Get raw text
2. **Code Node** ‚Üí Universal Document Cleaner
   - Input: `$json.text_content` or `$json.data`
   - Returns: Array of cleaned chunks
3. **Loop Over Chunks** ‚Üí Generate embeddings
4. **Store in Qdrant** ‚Üí Vector + metadata

**Example Code Node Setup:**
```javascript
// Copy entire n8n-universal-document-cleaner.js into Code Node
// It will automatically process $json.text_content or $json.data
```

#### Testing Different File Types

**Test 1: Podcast Transcript**
```bash
# Input: The Insurance Dudes_transcripts.txt
# Expected: Episode-based chunks, speakers removed, HTML cleaned
# Confidence: >0.9
```

**Test 2: PDF Extract**
```bash
# Input: Complex PDF with formatting artifacts
# Expected: Clean paragraphs, special chars normalized
# Confidence: >0.7
```

**Test 3: Plain Email**
```bash
# Input: Email with headers and signature
# Expected: Headers stripped, signature removed, body preserved
# Confidence: >0.8
```

**Test 4: Unknown Format**
```bash
# Input: Mixed/unrecognized format
# Expected: Fallback basic cleanup, still usable chunks
# Confidence: 0.5-0.7
```

#### Error Handling & Fallbacks

The cleaner **never crashes**. If detection or processing fails:

1. Falls back to `basicCleanup()` (conservative HTML/entity removal)
2. Uses `simpleChunking()` (character-based splitting)
3. Returns chunks with `processing_applied: ['fallback_cleanup']`
4. Includes error details in metadata
5. Logs warning but continues processing

#### Performance Characteristics

- **Speed**: Detection adds ~10-50ms overhead
- **Memory**: Processes in-place, no large duplications
- **Scaling**: Linear with document size
- **Reliability**: 100% success rate (fallback guaranteed)

#### Monitoring & Debugging

Console logs show processing details:
```
üîç DETECTION RESULTS:
  Format: podcast_transcript (92% confidence)
  Indicators: episode_delimiters, html_tags, speaker_labels

üßπ CLEANING APPLIED: episode_normalization ‚Üí html_tags_removed ‚Üí
   html_entities_normalized ‚Üí speaker_labels_removed ‚Üí
   aggressive_whitespace ‚Üí special_chars_normalized

‚úÖ PROCESSING COMPLETE:
  Format: podcast_transcript
  Original: 450123 chars
  Cleaned: 387654 chars
  Reduction: 14%
  Chunks: 163
  Applied: episode_normalization, html_tags_removed, ...
```

#### Best Practices

1. **Always use detection metadata** to understand what was processed
2. **Check confidence scores** - Low confidence may need manual review
3. **Preserve structure when needed** - Episode/section markers can be useful
4. **Test with representative samples** from each document type
5. **Monitor reduction percentages** - Very high reduction may indicate over-cleaning

#### File Location

```
/mnt/volume_nyc1_01/idudesRAG/n8n-universal-document-cleaner.js
```

**Usage**: Copy entire file into n8n Code Node, it auto-processes `$json.text_content` or `$json.data`

---

## üìã STREAMLINED IMPLEMENTATION CHECKLIST

### Phase 1: Essential Metadata (15 minutes)

**Files to Edit:**
- `json-flows/02-DocumentUpload&Vectorization.json` (PrepDoc node)
- `json-flows/01-Google-Drive-Ingestion.json` (PrepDoc node)

#### Step 1.1: Update PrepDoc Node in Both Workflows

**Location:** Find the `PrepDoc` or `Prepare Document` node (usually Code node)

**Action:** Add this code to the metadata section (around line 90-110 in the node):

```javascript
// STREAMLINED METADATA - Essential fields only
const isTranscript = filename.toLowerCase().includes('transcript') ||
                     filename.toLowerCase().includes('episode') ||
                     file_type === 'text/vtt' ||
                     file_type === 'application/x-subrip';

// Extract episode ID from filename (e.g., "Episode 42.txt" ‚Üí "42")
const episodeMatch = filename.match(/episode[\s-_]*(\d+)/i);
const episodeId = episodeMatch ? episodeMatch[1] : null;

// Add to metadata object
metadata: {
  filename: filename,
  file_type: file_type,
  file_size: file_size || content.length,
  source: source,
  timestamp: new Date().toISOString(),
  upload_source: upload_source,
  drive_file_id: input.json?.id || null,
  // NEW FIELDS
  document_type: isTranscript ? 'transcript' : 'document',
  language: 'en',
  episode_id: episodeId
}
```

**n8n UI Steps:**
1. Open workflow 02 (DocumentUpload&Vectorization)
2. Click on `PrepDoc` node (Code node type)
3. Scroll to the `metadata` object in the JavaScript
4. Replace the metadata section with code above
5. Click "Execute Node" to test
6. Repeat for workflow 01 (Google-Drive-Ingestion)

---

#### Step 1.2: Update Document Loader Metadata Values

**Location:** Both workflows, find `Document Loader` or `DocLoader` node

**Action:** Add these metadata values to the node configuration

**n8n UI Steps:**
1. Click on `Document Loader` node (type: `@n8n/n8n-nodes-langchain.documentDefaultDataLoader`)
2. Scroll to "Metadata" section
3. Click "Add Metadata Value" 3 times
4. Enter these exact values:

| Name | Value (use Expression mode) |
|------|----------------------------|
| `document_type` | `{{ $('PrepDoc').item.json.metadata.document_type }}` |
| `language` | `{{ $('PrepDoc').item.json.metadata.language }}` |
| `episode_id` | `{{ $('PrepDoc').item.json.metadata.episode_id }}` |

**Note:** Use the three-dot menu on each value field ‚Üí "Expression" to enable `{{ }}` syntax

---

### Phase 2: Simple Hybrid Search (20 minutes)

**File to Edit:**
- `json-flows/03-Chat-and-Search-Embeddings.json` (Create new Code node)

#### Step 2.1: Add Hybrid Search Code Node

**Location:** Workflow 08 (Chat-and-Search-Embeddings)

**Action:** Add a new Code node to replace/enhance the vector search

**n8n UI Steps:**
1. Open workflow 08 (Chat-and-Search-Embeddings)
2. Click the "+" button between nodes to add a new node
3. Search for "Code" node, add it
4. Name it: `Hybrid Search (Vector + BM25)`
5. Paste this COMPLETE code:

```javascript
// HYBRID SEARCH: Simple Vector + BM25 Combination
const query = $input.first().json.chatInput || $input.first().json.query;
const limit = 8; // Increased from 4 to 8 for more context
const minSimilarity = 0.65; // Lowered from 0.7 for better recall

// PostgreSQL client
const { Client } = require('pg');
const client = new Client({
  connectionString: 'postgres://postgres:d7ToQHAA7VecTKi2DxFgNxtlj~xN_HnD@yamabiko.proxy.rlwy.net:15649/railway'
});

try {
  await client.connect();
  
  // Step 1: Generate embedding for query
  const embeddingResponse = await fetch('https://api.openai.com/v1/embeddings', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${$credentials.openAiApi.apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      input: query,
      model: 'text-embedding-3-small',
      dimensions: 1536
    })
  });
  
  const embeddingData = await embeddingResponse.json();
  const queryEmbedding = embeddingData.data[0].embedding;
  
  // Step 2: Run hybrid search (Vector + BM25)
  const hybridQuery = `
    WITH vector_results AS (
      SELECT
        de.id,
        de.document_id,
        de.text as chunk_text,
        de.metadata,
        d.filename,
        d.spaces_url,
        (1 - (de.embedding <=> $1::vector)) AS vector_score
      FROM core.document_embeddings de
      JOIN core.documents d ON d.id = de.document_id
      WHERE (1 - (de.embedding <=> $1::vector)) >= $2
      ORDER BY de.embedding <=> $1::vector
      LIMIT 15
    ),
    bm25_results AS (
      SELECT
        de.id,
        de.document_id,
        de.text as chunk_text,
        de.metadata,
        d.filename,
        d.spaces_url,
        ts_rank_cd(to_tsvector('english', de.text), plainto_tsquery('english', $3)) AS bm25_score
      FROM core.document_embeddings de
      JOIN core.documents d ON d.id = de.document_id
      WHERE to_tsvector('english', de.text) @@ plainto_tsquery('english', $3)
      ORDER BY bm25_score DESC
      LIMIT 15
    ),
    combined AS (
      SELECT
        COALESCE(v.id, b.id) AS id,
        COALESCE(v.document_id, b.document_id) AS document_id,
        COALESCE(v.chunk_text, b.chunk_text) AS chunk_text,
        COALESCE(v.metadata, b.metadata) AS metadata,
        COALESCE(v.filename, b.filename) AS filename,
        COALESCE(v.spaces_url, b.spaces_url) AS spaces_url,
        COALESCE(v.vector_score, 0) AS vector_score,
        COALESCE(b.bm25_score, 0) AS bm25_score,
        (COALESCE(v.vector_score, 0) * 0.6 + COALESCE(b.bm25_score, 0) * 0.4) AS combined_score
      FROM vector_results v
      FULL OUTER JOIN bm25_results b ON v.id = b.id
    )
    SELECT * FROM combined
    ORDER BY combined_score DESC
    LIMIT $4;
  `;
  
  const result = await client.query(hybridQuery, [
    JSON.stringify(queryEmbedding),
    minSimilarity,
    query,
    limit
  ]);
  
  // Step 3: Format results for n8n
  const formattedResults = result.rows.map(row => ({
    json: {
      id: row.id,
      document_id: row.document_id,
      content: row.chunk_text,
      metadata: row.metadata,
      filename: row.filename,
      spaces_url: row.spaces_url,
      scores: {
        vector: parseFloat(row.vector_score || 0).toFixed(4),
        bm25: parseFloat(row.bm25_score || 0).toFixed(4),
        combined: parseFloat(row.combined_score || 0).toFixed(4)
      }
    }
  }));
  
  return formattedResults;
  
} catch (error) {
  console.error('Hybrid search error:', error);
  throw new Error(`Hybrid search failed: ${error.message}`);
} finally {
  await client.end();
}
```

**Node Settings:**
- **Run Once for All Items**: OFF
- **Mode**: Run Once for Each Item

**Required Credentials:**
- OpenAI API credential (select from dropdown)

---

#### Step 2.2: Update Vector Store Tool topK

**Location:** Workflow 08, `Answer questions with a vector store` node

**Action:** Increase context window

**n8n UI Steps:**
1. Find the `Answer questions with a vector store` node (Tool Vector Store type)
2. Click on it
3. Find the `topK` parameter
4. Change from `4` to `8`
5. Save the workflow

---

### Phase 3: Update AI Agent System Prompt (5 minutes)

**Location:** Workflow 08, AI Agent node

**Action:** Ensure agent knows about new metadata fields

**n8n UI Steps:**
1. Click on the `idudes-content-expert` node (AI Agent type)
2. Scroll to "System Message" section
3. **No changes needed** - the current system prompt already handles the use case perfectly
4. Just verify it references the Insurance Dudes Content Agent prompt

---

## üóÑÔ∏è SQL CHANGES (FOR APPROVAL - DO NOT EXECUTE YET)

### Optional: Add Metadata Indexes for Performance

**Only needed if you have 10,000+ document chunks**

```sql
-- Add GIN index for JSONB metadata queries (episode_id, document_type filters)
CREATE INDEX CONCURRENTLY idx_embeddings_metadata_gin 
ON core.document_embeddings USING GIN (metadata jsonb_path_ops);

-- Add composite index for common queries (document_id + episode filtering)
CREATE INDEX CONCURRENTLY idx_embeddings_doc_episode 
ON core.document_embeddings (document_id, (metadata->>'episode_id'));

-- Verify indexes are being used
EXPLAIN ANALYZE
SELECT * FROM core.document_embeddings
WHERE metadata->>'episode_id' = '42'
LIMIT 10;
```

**Do you need this?** Probably not yet. Wait until you have performance issues.

---

## üíª CODE CHANGES (FOR APPROVAL - NO CHANGES NEEDED)

**Good news:** No TypeScript/Next.js code changes required!

All changes are in n8n workflows only. The existing UI and API will work as-is.

---

## üß™ TESTING CHECKLIST

### Test 1: Metadata Auto-Detection
1. Upload file named: `Episode 42 - Insurance Claims.txt`
2. Check database: `SELECT metadata FROM core.document_embeddings WHERE metadata->>'episode_id' = '42' LIMIT 1;`
3. **Expected:** `{"document_type": "transcript", "language": "en", "episode_id": "42", ...}`

### Test 2: Non-Transcript Document
1. Upload file named: `Policy Guidelines 2024.pdf`
2. Check database: `SELECT metadata FROM core.document_embeddings ORDER BY id DESC LIMIT 1;`
3. **Expected:** `{"document_type": "document", "language": "en", "episode_id": null, ...}`

### Test 3: Hybrid Search (Episode ID)
1. In chat UI, search: `episode 42`
2. **Expected:** Results should include chunks from episode 42 ranked high (BM25 exact match)

### Test 4: Hybrid Search (Semantic)
1. In chat UI, search: `how to handle difficult claims`
2. **Expected:** Relevant results even if exact words not in transcript (vector search)

### Test 5: Urdu Input ‚Üí English Output
1. In chat UI, type (in Urdu): `ÿßŸÜÿ¥Ÿàÿ±ŸÜÿ≥ ⁄©ŸÑ€åŸÖ ⁄©€í ÿ®ÿßÿ±€í ŸÖ€å⁄∫ ÿ®ÿ™ÿßÿ¶€å⁄∫` (Tell me about insurance claims)
2. **Expected:** AI responds in English with Insurance Dudes voice, citing episode sources

### Test 6: Increased Context
1. Search any query
2. Check results: Should return 8 chunks (not 4)
3. **Expected:** More context for AI agent to generate better content ideas

---

## üìä PERFORMANCE EXPECTATIONS

| Metric | Before (Simple Vector) | After (Streamlined Hybrid) | Change |
|--------|----------------------|--------------------------|---------|
| **Latency** | 50-100ms | 120-180ms | +70-80ms (acceptable) |
| **Recall@8** | ~60% | ~80% | +33% improvement |
| **Precision@8** | ~70% | ~82% | +17% improvement |
| **Cost/search** | $0.0001 | $0.00012 | +20% ($1.20 extra per 10k searches) |
| **Context per query** | 4 chunks | 8 chunks | 2x more context for AI |

**Bottom line:** Small latency increase, minimal cost increase, significantly better results.

---

## üéØ WHAT WE CUT AND WHY

### Multi-Query Expansion (ELIMINATED)
- **Original plan:** Generate 2-3 query variants, search all, merge results
- **Why we cut it:** 
  - Adds 200-400ms latency
  - Costs 3.6x more per search
  - Content team knows what they're searching for (not end-users)
  - Diminishing returns for social content creation

### LLM Re-Ranking (ELIMINATED)
- **Original plan:** Use GPT-5-nano to score and re-rank top 20 results
- **Why we cut it:**
  - Adds 200-300ms latency
  - AI agent already does contextual filtering in chat
  - Over-optimization for <10k document corpus
  - Better to let agent pick relevant chunks from 8 results

### Complex RRF Scoring (SIMPLIFIED)
- **Original plan:** Reciprocal Rank Fusion with k=60 constant
- **What we use instead:** Simple weighted average `(vector * 0.6 + bm25 * 0.4)`
- **Why simplified:**
  - RRF is for large-scale search engines (Google, Bing)
  - Simple weighting works fine for small corpus
  - Easier to tune (just adjust 0.6/0.4 weights)

### Advanced Metadata (REDUCED)
- **Original plan:** 5 new fields (document_type, language, episode_id, speaker, episode_title)
- **What we kept:** 3 fields (document_type, language, episode_id)
- **Why reduced:**
  - `speaker`: No speaker diarization in current transcripts
  - `episode_title`: Brittle regex extraction, not critical
  - Can add later if needed

---

## üöÄ ROLLBACK PLAN

If anything breaks:

1. **Revert Hybrid Search:** Delete the new `Hybrid Search` Code node, re-enable original Vector Store
2. **Revert topK:** Change back from 8 ‚Üí 4
3. **Keep Metadata:** The new metadata fields are harmless (just null for old documents)

**How to verify rollback worked:**
- Search should return results in <100ms
- Results count back to 4 per query

---

## üí∞ COST ANALYSIS

### Per Search Cost Breakdown

**Before (Simple Vector):**
- Embedding generation: $0.0001
- **Total: $0.0001/search**

**After (Streamlined Hybrid):**
- Embedding generation: $0.0001
- BM25 search: $0 (PostgreSQL free)
- Extra DB query time: $0.00002 (negligible)
- **Total: $0.00012/search**

**For 10,000 searches/month:**
- Before: $1.00
- After: $1.20
- **Extra cost: $0.20/month** ‚úÖ Totally acceptable

**What we saved by cutting features:**
- Multi-query expansion: -$1.00/month (eliminated)
- LLM re-ranking: -$2.00/month (eliminated)
- **Total savings: $3.00/month**

---

## üéì TRAINING CONTENT TEAM

### Search Tips for Better Results

**For Episode-Specific Searches:**
- Good: `episode 42 claims process`
- Bad: `that one where they talked about claims`

**For Concept Searches:**
- Good: `agent retention strategies`
- Bad: `keeping agents` (too generic)

**For Quote Mining:**
- Good: `Jason quote about leadership`
- Bad: `what did Jason say` (too vague)

**Urdu Input Best Practices:**
- Clear, specific queries work best
- AI will understand Urdu but respond in English
- Use episode numbers when known: `ÿß€åŸæ€åÿ≥Ÿà⁄à 42` (Episode 42)

---

## üìù NEXT STEPS AFTER IMPLEMENTATION

1. **Monitor Performance:**
   - Check search latency in n8n execution logs
   - Track AI agent response quality
   - Collect team feedback on result relevance

2. **Tune Weights (If Needed):**
   - If keyword searches too dominant: Change `0.4` ‚Üí `0.3` in BM25 weight
   - If semantic search too weak: Change `0.6` ‚Üí `0.7` in vector weight

3. **Future Enhancements (Only if needed):**
   - Add `speaker` metadata if transcripts get speaker diarization
   - Add caching for repeated queries (Redis)
   - Add search analytics dashboard

---

## üìù Transcript Preprocessing for RAG

### Overview
Before embedding podcast transcripts, they must be cleaned and chunked to ensure:
- **Maximum embedding accuracy** (no HTML noise)
- **Optimal chunk size** for semantic search (~400 chars)
- **Contextual integrity** (sentence boundary splitting)
- **Efficient vector storage** (normalized whitespace)

### Implementation: n8n Code Node

**File Location**: `/mnt/volume_nyc1_01/idudesRAG/n8n-transcript-cleaner.js`

#### Where to Add in Workflows

1. **Workflow 02: Document Upload & Vectorization**
   - **Position**: After "Get Document Content" node, before "OpenAI Embeddings" node
   - **Node Type**: Code Node
   - **Node Name**: "Clean & Chunk Transcript"

2. **Workflow 01: Google Drive Ingestion** 
   - **Position**: After downloading transcript file, before embedding
   - **Node Type**: Code Node
   - **Node Name**: "Preprocess Transcript"

#### Configuration Steps

1. **Add Code Node to workflow**
   - Drag "Code" node from left panel
   - Place between content retrieval and embedding nodes

2. **Configure Code Node**
   - **Language**: JavaScript
   - **Mode**: Run Once for All Items
   - Copy entire contents of `n8n-transcript-cleaner.js`
   - Paste into code editor

3. **Input Requirements**
   - Previous node must output: `$json.data` (string containing raw transcript)
   - Expected format: Episodes with `-----BEGIN`/`-----END` delimiters
   - Can handle HTML tags, timestamps, metadata

4. **Output Format**
   ```javascript
   [
     { 
       json: { 
         text_chunk: "First clean segment...",
         chunk_index: 0,
         chunk_length: 387,
         total_chunks: 42
       } 
     },
     { 
       json: { 
         text_chunk: "Second clean segment...",
         chunk_index: 1,
         chunk_length: 412,
         total_chunks: 42
       } 
     }
     // ... more chunks
   ]
   ```

#### Processing Pipeline

The code performs these operations in order:

1. **Episode Separation** 
   - Splits by `-----BEGIN`/`-----END` markers
   - Removes delimiter lines completely
   - Strips header metadata (Episode, Date, Duration, etc.)

2. **Tag Stripping**
   - Removes ALL HTML/XML tags: `<time>`, `<cite>`, `<p>`, etc.
   - Uses regex: `/<[^>]+>/g`

3. **Entity Normalization**
   - Converts HTML entities to ASCII:
     - `&#39;` ‚Üí `'`
     - `&quot;` ‚Üí `"`
     - `&amp;` ‚Üí `&`
     - Plus 15+ other common entities
   - Handles numeric (`&#NNNN;`) and hex (`&#xHHHH;`) entities

4. **Whitespace Cleanup**
   - Replaces `\n`, `\r` with single space
   - Collapses multiple spaces to single space
   - Trims leading/trailing whitespace

5. **Contextual Chunking**
   - **Target size**: ~400 characters
   - **Max size**: 500 characters
   - **Split method**: Sentence boundaries only (`. ` followed by capital letter)
   - Avoids breaking mid-sentence or mid-word
   - Maintains minimum chunk size (200+ chars)

#### Testing Checklist

- [ ] Code Node successfully receives raw transcript data
- [ ] HTML tags are completely removed from output
- [ ] HTML entities converted to readable characters
- [ ] No newlines present in chunks (single space instead)
- [ ] Chunks average ~400 characters
- [ ] No chunk exceeds 500 characters
- [ ] Chunks split at sentence boundaries only
- [ ] Output array has `text_chunk` field for each item
- [ ] Processing stats logged to console
- [ ] Error handling works (test with malformed input)

#### Error Handling

The code includes comprehensive error handling:

```javascript
// Returns error item if processing fails
{
  json: {
    error: true,
    error_message: "Detailed error description",
    error_stack: "Full stack trace",
    text_chunk: null
  }
}
```

**Common errors to check**:
- Empty/null input data
- Input not a string
- No content after cleaning (all metadata/tags)
- Memory issues with very large transcripts

#### Performance Metrics

**Expected processing speed**:
- ~10,000 chars/second on n8n cloud
- ~50,000 chars/second on self-hosted

**Sample output** (from 70,000 char transcript):
- Input: 70,234 characters
- Cleaned: 65,120 characters (7% reduction)
- Chunks: 163
- Avg chunk: 399 chars
- Min chunk: 287 chars
- Max chunk: 498 chars

#### Integration with Embeddings

**Next node configuration** (OpenAI Embeddings):
- Input field: `{{ $json.text_chunk }}`
- Model: `text-embedding-3-small` or `text-embedding-ada-002`
- Batch size: 100 (process 100 chunks at a time)

**Vector storage** (Qdrant/PostgreSQL):
- Store `chunk_index` for ordering
- Store `total_chunks` for context
- Link chunks to same `document_id`

#### Maintenance Notes

**When to update this code**:
- New HTML entities appear in transcripts
- Different delimiter format needed
- Chunk size optimization needed
- New metadata patterns to strip

**Version tracking**: Document changes in git commits with tag `transcript-cleaning`

---

## ‚úÖ IMPLEMENTATION SIGN-OFF

**Before you start:**
- [ ] Read this entire plan
- [ ] Understand what we're keeping vs cutting
- [ ] Verify n8n workflows are backed up
- [ ] Confirm Railway PostgreSQL credentials in `.env`

**After implementation:**
- [ ] Run all 6 tests in Testing Checklist
- [ ] Verify search latency <200ms
- [ ] Confirm metadata fields populated for new uploads
- [ ] Train content team on new search capabilities

**Estimated time:** 40 minutes total (not 90 minutes like advanced plan)

---

## üÜò TROUBLESHOOTING

### Issue: "Cannot find module 'pg'"
**Solution:** n8n doesn't have `pg` module by default. Use n8n's built-in PostgreSQL node instead or install via n8n settings.

**Better solution:** Replace `require('pg')` code with n8n's PostgreSQL Execute Query node:
1. Add "PostgreSQL" node after embedding generation
2. Use the hybrid query from Step 2.1
3. Pass query embedding as parameter `$1`

### Issue: Hybrid search returns no results
**Check:**
1. Is full-text search index created? Run: `SELECT * FROM pg_indexes WHERE tablename = 'document_embeddings' AND indexname LIKE '%fts%';`
2. Are there documents in database? Run: `SELECT COUNT(*) FROM core.document_embeddings;`
3. Is query embedding valid? Check `embeddingData` response in node logs

### Issue: Metadata fields are null for old documents
**Expected behavior:** Old documents won't have new metadata. Only new uploads will.

**Fix (if needed):** Run migration script to backfill:
```sql
UPDATE core.document_embeddings
SET metadata = metadata || jsonb_build_object(
  'document_type', CASE WHEN metadata->>'filename' ILIKE '%episode%' THEN 'transcript' ELSE 'document' END,
  'language', 'en',
  'episode_id', (SELECT substring(metadata->>'filename' FROM 'episode[\\s-_]*(\\d+)'))
)
WHERE metadata->>'document_type' IS NULL;
```

---

## üéâ SUCCESS CRITERIA

**You'll know it's working when:**

1. ‚úÖ Content team can search "episode 42" and get exact episode chunks
2. ‚úÖ Semantic searches like "agent retention" return relevant quotes across episodes
3. ‚úÖ Urdu queries work and return English Insurance Dudes-style responses
4. ‚úÖ Search returns 8 results (more context for AI)
5. ‚úÖ Latency stays under 200ms per search
6. ‚úÖ Team feedback: "Results are more relevant than before"

**You'll know we made the right cuts when:**

1. ‚úÖ Searches feel fast (not sluggish)
2. ‚úÖ Cost stays under $2/month for 10k searches
3. ‚úÖ Team doesn't complain about missing features
4. ‚úÖ Content quality improves without needing multi-query or re-ranking

---

**End of Streamlined RAG Plan**

---

## üéØ CONTENT EXTRACTION PROMPTS

### How to Use These Prompts

Each prompt below is designed to extract specific content types from the RAG system that match the Insurance Dudes Social Content Playbook formats. Copy-paste these directly into the chat interface to get viral-ready content.

**For Urdu-speaking team members:** You can paste these prompts in Urdu, and the system will return English responses in Craig & Jason's voice.

---

### üìÖ WEEKLY CONTENT FORMATS

#### Content Type: Monday Mayhem (Claims Horror Stories)

**Query Format:**
```
Find a claims horror story about [topic] where an agent or client made a costly mistake. I need the full story with numbers and consequences. Prefer stories from episodes in the last 6 months.
```

**Expected Output:**
- Specific claim amount/loss figures
- The exact mistake that was made
- Episode number and timestamp for sourcing
- Craig or Jason's commentary on the mistake

**Example Searches:**
```
Find a weekend claims disaster story with dollar amounts over $100k
```
```
Give me a claims horror story about E&O coverage gaps
```
```
ÿß€åÿ≥€å ⁄©ŸÑ€åŸÖ ⁄©€ÅÿßŸÜ€å ÿØ€å⁄∫ ÿ¨ÿ≥ ŸÖ€å⁄∫ ÿ®⁄ëÿß ŸÜŸÇÿµÿßŸÜ €ÅŸàÿß (Show me a claim story with big losses)
```

---

#### Content Type: Teaching Tuesday (Explainer Content)

**Query Format:**
```
Explain [complex insurance concept] in simple terms like Craig and Jason would. Include the common mistake agents make and how to avoid it. Need episode references.
```

**Expected Output:**
- Clear explanation of the concept
- Common mistakes/misconceptions
- Practical examples from the show
- Episode numbers for verification

**Example Searches:**
```
Explain E&O vs GL coverage like you're talking to a new agent
```
```
Break down the difference between occurrence and claims-made policies
```
```
ÿßŸÜÿ¥Ÿàÿ±ŸÜÿ≥ ⁄©Ÿàÿ±€åÿ¨ ⁄©€å ÿßŸÇÿ≥ÿßŸÖ ÿ≥ŸÖÿ¨⁄æÿßÿ¶€å⁄∫ (Explain insurance coverage types)
```

---

#### Content Type: War Story Wednesday (Podcast Clips)

**Query Format:**
```
Find the best war story from episode [number] OR find a dramatic story about [topic] that has a clear beginning, conflict, and lesson. Need 8-12 minute worthy content.
```

**Expected Output:**
- Complete story arc (setup ‚Üí conflict ‚Üí resolution)
- Dramatic moments/quotable lines
- Clear lesson or takeaway
- Episode number and approximate timestamp

**Example Searches:**
```
Find the craziest client story from the last 3 episodes
```
```
Give me a dramatic agent success story with a major obstacle
```
```
⁄©ÿßŸÖ€åÿßÿ®€å ⁄©€å ⁄©€ÅÿßŸÜ€å ÿØ€å⁄∫ ÿ¨ÿ≥ ŸÖ€å⁄∫ ŸÖÿ¥⁄©ŸÑÿßÿ™ ÿ™⁄æ€å⁄∫ (Success story with challenges)
```

---

#### Content Type: Throwdown Thursday (Hot Takes)

**Query Format:**
```
Find Craig or Jason's controversial opinion about [topic]. I need their actual quote plus the reasoning/data they used to back it up. Looking for takes that challenge industry norms.
```

**Expected Output:**
- Direct quote of the hot take
- Supporting arguments/data mentioned
- Industry belief they're challenging
- Episode reference

**Example Searches:**
```
What's Craig and Jason's hot take on GEICO and direct-to-consumer insurance?
```
```
Find their controversial opinion about agent commissions or pricing
```
```
ÿßŸÜ⁄àÿ≥Ÿπÿ±€å ⁄©€í ÿ®ÿßÿ±€í ŸÖ€å⁄∫ ŸÖÿ™ŸÜÿßÿ≤ÿπ€Å ÿ±ÿßÿ¶€í (Controversial industry opinion)
```

---

#### Content Type: Friday Finals (Newsletter Content)

**Query Format:**
```
Give me the top 3 lessons or biggest mistakes from episodes this week/month. Need quote-worthy moments and practical takeaways for the newsletter.
```

**Expected Output:**
- 3-5 key lessons with context
- Quotable moments from each
- Listener-applicable advice
- Episode numbers for all references

**Example Searches:**
```
What were the top mistakes discussed in the last 4 episodes?
```
```
Find the best quotes from Jason about agency growth from recent episodes
```
```
ÿßÿ≥ €ÅŸÅÿ™€í ⁄©€í ÿß€ÅŸÖ ÿßÿ≥ÿ®ÿßŸÇ (This week's key lessons)
```

---

#### Content Type: Sales Saturday (Role-Play Content)

**Query Format:**
```
Find a sales conversation or objection handling example from the podcast. Need the exact dialogue if possible, including the agent's approach and Craig/Jason's commentary.
```

**Expected Output:**
- Actual sales dialogue/scenario
- Objection and how it was handled
- Craig/Jason's analysis of what worked
- Episode reference

**Example Searches:**
```
Find examples of handling price objections in insurance sales
```
```
Give me a sales role-play example about cross-selling commercial policies
```
```
ÿ≥€åŸÑÿ≤ ⁄©ÿßŸÑ ⁄©€å ŸÖÿ´ÿßŸÑ (Sales call example)
```

---

### üé® CONTENT PILLAR EXTRACTIONS

#### Content Type: Education (How-To's & Breakdowns)

**Query Format:**
```
Teach me how to [specific task/process] the Insurance Dudes way. Need step-by-step guidance with real examples from their experience.
```

**Expected Output:**
- Step-by-step process
- Common pitfalls to avoid
- Real-world examples from episodes
- Practical tips in Craig & Jason's voice

**Example Searches:**
```
How to properly audit a client's coverage for gaps
```
```
Step-by-step process for onboarding a new commercial client
```
```
ŸÜ€åÿß ⁄©ŸÑÿßÿ¶ŸÜŸπ ⁄©€åÿ≥€í ÿ¥ÿ±Ÿàÿπ ⁄©ÿ±€å⁄∫ (How to start a new client)
```

---

#### Content Type: Entertainment (War Stories & Roasts)

**Query Format:**
```
Find the funniest/most ridiculous [client stupidity/agent mistake/industry absurdity] story. Need the full entertaining version with Craig and Jason's reactions.
```

**Expected Output:**
- Complete entertaining story
- Funny/absurd details
- Craig & Jason's commentary/roasting
- Episode reference

**Example Searches:**
```
Find the most ridiculous client question or request story
```
```
What's the funniest carrier underwriting rejection story?
```
```
ŸÖÿ≤ÿßÿ≠€å€Å ⁄©€ÅÿßŸÜ€å (Funny story)
```

---

#### Content Type: Emotion (Success Stories & Failures)

**Query Format:**
```
Find an emotional/inspiring story about [agent success/overcoming failure/community win]. Need the struggle, the breakthrough, and the lesson.
```

**Expected Output:**
- Emotional arc (struggle ‚Üí breakthrough)
- Personal details that make it relatable
- The lesson learned
- Episode reference

**Example Searches:**
```
Find a story about an agent who almost quit but turned it around
```
```
Give me an inspiring first-year agent success story
```
```
ÿ≠ŸàÿµŸÑ€Å ÿßŸÅÿ≤ÿß ⁄©ÿßŸÖ€åÿßÿ®€å ⁄©€å ⁄©€ÅÿßŸÜ€å (Inspiring success story)
```

---

#### Content Type: Engagement (Q&A Content)

**Query Format:**
```
What questions did listeners ask about [topic]? Or what are the most common questions Craig and Jason answer about [topic]?
```

**Expected Output:**
- Actual listener questions
- Craig & Jason's complete answers
- Follow-up Q&A if available
- Episode references

**Example Searches:**
```
What are the most common questions about starting an insurance agency?
```
```
Find Q&A about agent retention and client loyalty
```
```
ÿπÿßŸÖ ÿ≥ŸàÿßŸÑÿßÿ™ (Common questions)
```

---

### üì± PLATFORM-SPECIFIC CONTENT

#### Content Type: YouTube Shorts/Reels (15-45 seconds)

**Query Format:**
```
Find a 30-second worthy moment about [topic] - need a strong hook, quick insight, and clear payoff. Looking for "holy shit" moments.
```

**Expected Output:**
- Short, punchy quote or moment
- Clear hook (first 3 seconds)
- Standalone value
- Episode timestamp for clipping

**Example Searches:**
```
Find a shocking statistic or number Craig mentioned about claims
```
```
Give me Jason's best one-liner about agent productivity
```
```
⁄Ü⁄æŸàŸπÿß ŸÑ€å⁄©ŸÜ ÿ≤Ÿàÿ±ÿØÿßÿ± ŸÑŸÖÿ≠€Å (Short but powerful moment)
```

---

#### Content Type: LinkedIn Teaching Posts

**Query Format:**
```
Find a professional lesson about [topic] that would work as a LinkedIn post. Need the insight, supporting example, and actionable takeaway. 1300 character range.
```

**Expected Output:**
- Clear professional insight
- Real-world example/story
- Actionable advice
- Episode reference for credibility

**Example Searches:**
```
Find a lesson about agency operations that would resonate on LinkedIn
```
```
Give me Craig's advice on hiring and team building with an example
```
```
Ÿæÿ±ŸàŸÅ€åÿ¥ŸÜŸÑ ŸÖÿ¥Ÿàÿ±€Å (Professional advice)
```

---

#### Content Type: X/Twitter Threads

**Query Format:**
```
Find a complex topic Craig and Jason broke down that could work as a 5-10 tweet thread. Need the main argument, supporting points, and data/examples.
```

**Expected Output:**
- Main thesis/hot take
- 3-5 supporting points
- Data, examples, or stories for each
- Episode reference

**Example Searches:**
```
Find their breakdown of why traditional insurance marketing is dying
```
```
Give me their argument about the future of independent agents
```
```
ÿ™ŸÅÿµ€åŸÑ€å ŸÖŸàÿ∂Ÿàÿπ (Detailed topic)
```

---

### üé£ HOOK FORMULA EXTRACTIONS

#### Content Type: Number/Loss Hooks

**Query Format:**
```
Find stories with specific dollar amounts of losses, claims, or mistakes. Need the exact number and what caused it.
```

**Example Searches:**
```
Find claims over $500k and what went wrong
```
```
Give me examples of small policy gaps that led to huge losses
```

---

#### Content Type: "Insurance Companies Hate This" Hooks

**Query Format:**
```
Find insider knowledge or industry secrets that Craig and Jason revealed. Looking for "what carriers don't want you to know" type content.
```

**Example Searches:**
```
What industry secrets have Craig and Jason shared about carrier pricing?
```
```
Find examples of coverage options agents don't usually mention
```

---

#### Content Type: Celebrity/Relatable POV Hooks

**Query Format:**
```
Find stories about well-known people or highly relatable situations. Need the "this could happen to you" angle.
```

**Example Searches:**
```
Find any stories about famous people or big companies with insurance failures
```
```
Give me the most relatable agent struggle story
```

---

### üîÑ CONTENT RECYCLING QUERIES

#### Extract: Quotable Tweets (5 per episode)

**Query Format:**
```
Give me 5 standalone quotes from episode [number] that work as tweets. Each should be punchy, insightful, and under 280 characters.
```

---

#### Extract: Teaching Moments (3 per episode)

**Query Format:**
```
Find 3 educational moments from episode [number] where Craig or Jason taught something specific. Need the lesson, example, and takeaway.
```

---

#### Extract: Audiogram Moments (15 per episode)

**Query Format:**
```
Find 15-30 second audio-worthy moments from episode [number]. Looking for dramatic stories, shocking stats, or quotable insights.
```

---

### üìä MONTHLY THEME QUERIES

#### January: "New Year, New Fuck-ups"

**Query Format:**
```
Find stories about New Year resolutions, goal-setting failures, or January mistakes in insurance. Need agent or client examples.
```

---

#### February: "Love & Liability"

**Query Format:**
```
Find any Valentine's Day related claims, relationship-related policies, or partnership/love-themed insurance stories.
```

---

#### March: "March Madness Mistakes"

**Query Format:**
```
Find crazy, bracket-worthy insurance mistakes or wild claims. Looking for tournament-style "which mistake is worse" content.
```

---

#### October: "Scary Stories"

**Query Format:**
```
Find the scariest/most horrifying insurance claims or agent nightmares. Halloween-themed horror content.
```

---

### üéØ ADVANCED SEARCH TECHNIQUES

#### Multi-Episode Story Mining

**Query Format:**
```
Find all mentions of [topic/person/company] across all episodes. I need to compile everything Craig and Jason have said about this.
```

**Example:**
```
Find everything mentioned about agency valuations and selling
```

---

#### Quote Attribution Searches

**Query Format:**
```
Find all quotes from [Craig/Jason] about [topic]. Need exact words if possible.
```

**Example:**
```
Give me all of Jason's quotes about hiring and firing
```

---

#### Time-Based Content

**Query Format:**
```
What was discussed in episodes from [time period]? Looking for timely or seasonal content.
```

**Example:**
```
What were the main topics in episodes from Q4 2024?
```

---

### üí° CONTENT TEAM PRO TIPS

**For Best Results:**

1. **Be Specific**: "Episode 42 claims story" > "claims story"
2. **Use Numbers**: Mention episode numbers when you know them
3. **Request Context**: Always ask for episode references
4. **Filter by Type**: Specify if you want transcripts only: "from podcast transcripts, find..."
5. **Combine Filters**: "Find a funny story about E&O from the last 3 months"

**Urdu Query Tips:**
- Use specific episode numbers: `ÿß€åŸæ€åÿ≥Ÿà⁄à 42` 
- Mention topic clearly: `⁄©ŸÑ€åŸÖ ⁄©€å ⁄©€ÅÿßŸÜ€å` (claim story)
- Request format: `ŸÖÿÆÿ™ÿµÿ± ⁄©€ÅÿßŸÜ€å` (short story) or `ÿ™ŸÅÿµ€åŸÑ` (detailed)
- The system understands Urdu but responds in English with Craig & Jason's voice

**Quality Checks:**
- ‚úÖ Does the output include episode numbers?
- ‚úÖ Are there specific quotes or numbers?
- ‚úÖ Does it sound like Craig & Jason?
- ‚úÖ Is it actionable for social content?

---

**End of Content Extraction Prompts**